{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create question embedding\n",
    "def embed_questions(questions:List, location:str)->None:\n",
    "    try:\n",
    "        ques_embeddings = model.encode(questions)\n",
    "        np.save(location,ques_embeddings, allow_pickle=True, fix_imports=True)\n",
    "        print('Questions successfully embedded')\n",
    "    except Exception as e:\n",
    "        return f\"Error encountered {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('qna.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions successfully embedded\n"
     ]
    }
   ],
   "source": [
    "embed_questions(df['Questions'].to_list(),'ques_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "ques_embeddings = np.load('ques_embedding.npy', allow_pickle=True)\n",
    "\n",
    "# load qna data\n",
    "df = pd.read_csv('qna.csv')\n",
    "\n",
    "# embedded our query\n",
    "def get_similar_queries(query:str, ques_df:pd.DataFrame,\n",
    "                        threshold:int = 0.5)->dict:\n",
    "    try:\n",
    "        emb_query = model.encode(query)\n",
    "        scores = cosine_similarity([emb_query], ques_embeddings)\n",
    "        mask = scores > threshold\n",
    "        return ques_df[mask.reshape(-1,1)].to_dict(orient='records')\n",
    "    except Exception as err:\n",
    "        return f\"Error encountered : {err}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''Blueberry Pudding Recepie'''\n",
    "response = get_similar_queries(query,df,0.5)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Questions': 'What is a cloud-based data lake?',\n",
       "  'Answers': 'A cloud-based data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale.'}]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = '''Data Lake'''\n",
    "response = get_similar_queries(query,df, 0.5)\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a LLM to make the answers better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=\"XXXXXXXXXXXXXXX\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_answers(query):\n",
    "\n",
    "    prompt = '''\n",
    "                I will provide you with a context consisting of questions and answers.\n",
    "                You should use only the provided context to answer the query.\n",
    "                Do not use any outside knowledge or make up your own answers.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Query:\n",
    "                {query}\n",
    "\n",
    "                Instructions:\n",
    "                - If the query can be answered using the context, provide the answer.\n",
    "                - If the query cannot be answered using the context, respond with \"I do not have the required details to answer the query.\"\n",
    "                '''\n",
    "    response = get_similar_queries(query, df)\n",
    "    prompt_ = prompt.format(context = response, query = query)\n",
    "    llm_response = genai.chat(prompt=prompt_, temperature=0)\n",
    "    return llm_response.messages[-1]['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. It is a cloud-based data storage solution that can be used to store any type of data, including text, images, videos, and audio. Data lakes are often used by businesses to store and analyze large amounts of data.\n",
      "\n",
      "Data lakes are a relatively new technology, and there are a number of different vendors that offer data lake solutions. Some of the most popular data lake vendors include Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform.\n",
      "\n",
      "Data lakes offer a number of advantages over traditional data warehouses. First, data lakes are designed to store any type of data, including unstructured data. This makes them ideal for storing data from a variety of sources, such as social media, sensors, and IoT devices. Second, data lakes are designed to be scalable. This means that they can be easily expanded to store more data as your business grows. Third, data lakes are designed to be open and flexible. This means that you can use any tools or technologies you want to analyze your data.\n",
      "\n",
      "However, data lakes also have some disadvantages. First, data lakes can be difficult to manage. This is because they often contain large amounts of data that can be difficult to organize and analyze. Second, data lakes can be expensive to operate. This is because they require a lot of storage space and computing power. Third, data lakes can be a security risk. This is because they often contain sensitive data that could be accessed by unauthorized users.\n",
      "\n",
      "Overall, data lakes are a powerful tool that can be used to store and analyze large amounts of data. However, they can be difficult to manage and expensive to operate. It is important to weigh the advantages and disadvantages of data lakes before deciding if they are the right solution for your business.\n"
     ]
    }
   ],
   "source": [
    "print(generate_llm_answers('Data Lakes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have the required details to answer the query.\n"
     ]
    }
   ],
   "source": [
    "print(generate_llm_answers('Blueberry Pie Recepie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
